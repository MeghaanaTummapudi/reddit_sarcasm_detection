{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ! pip uninstall tensorflow\n# ! pip install swifter","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:54:39.682807Z","iopub.execute_input":"2021-12-07T18:54:39.683106Z","iopub.status.idle":"2021-12-07T18:54:39.686957Z","shell.execute_reply.started":"2021-12-07T18:54:39.683071Z","shell.execute_reply":"2021-12-07T18:54:39.685861Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:05:11.805134Z","iopub.execute_input":"2021-12-07T18:05:11.805994Z","iopub.status.idle":"2021-12-07T18:05:11.810999Z","shell.execute_reply.started":"2021-12-07T18:05:11.805942Z","shell.execute_reply":"2021-12-07T18:05:11.810311Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Importing libraries\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import imdb\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Conv1D,Dropout,LSTM,Bidirectional\nfrom tensorflow.keras.layers import MaxPooling1D\nfrom tensorflow.keras.layers import GlobalMaxPooling1D\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing import sequence\nfrom tensorflow.keras.preprocessing.text import Tokenizer,one_hot\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n\nimport pandas as pd\nimport numpy as np\nimport re,nltk,swifter\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2021-12-07T19:01:17.026626Z","iopub.execute_input":"2021-12-07T19:01:17.027220Z","iopub.status.idle":"2021-12-07T19:01:17.037344Z","shell.execute_reply.started":"2021-12-07T19:01:17.027179Z","shell.execute_reply":"2021-12-07T19:01:17.036524Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"tf.test.is_gpu_available()\ntf.config.list_physical_devices('GPU')","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:05:14.177887Z","iopub.execute_input":"2021-12-07T18:05:14.178146Z","iopub.status.idle":"2021-12-07T18:05:16.228372Z","shell.execute_reply.started":"2021-12-07T18:05:14.178109Z","shell.execute_reply":"2021-12-07T18:05:16.227452Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv(r'../input/sarcasm/train-balanced-sarcasm.csv')\ndf = df.fillna('')\ndf = df[['label','comment','author','score','created_utc','parent_comment']]\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:05:16.229883Z","iopub.execute_input":"2021-12-07T18:05:16.230431Z","iopub.status.idle":"2021-12-07T18:05:23.571124Z","shell.execute_reply.started":"2021-12-07T18:05:16.230381Z","shell.execute_reply":"2021-12-07T18:05:23.570411Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"stops = set(stopwords.words('english')) - {'no','not','nor','against','above','below','off','own'}\ndef clean_text(comment):\n    text = str(comment)\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',text)\n    text = re.sub(\"<.*?>\", \" \", text)\n    text = re.sub(r\"[0-9]+\",\" \",text)\n    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"n\\'t\", ' not',text)\n    text = text.replace('\\\\r', ' ')\n    text = text.replace('\\\\\"', ' ')\n    text = text.replace('\\\\n', ' ')\n    text = re.sub('[^A-Za-z0-9]+',' ', text)\n    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n    text = ' '.join(token for token in tokenizer.tokenize(text.lower()) if token not in stops)\n    text = text.lower().strip()\n    return text\n\ndf[\"cleaned_comment\"] = df.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:05:23.572372Z","iopub.execute_input":"2021-12-07T18:05:23.572728Z","iopub.status.idle":"2021-12-07T18:08:06.017388Z","shell.execute_reply.started":"2021-12-07T18:05:23.572688Z","shell.execute_reply":"2021-12-07T18:08:06.016332Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"train_x, val_x,train_y , val_y = train_test_split(df.drop('label',axis=1),df['label'],random_state=123,test_size=0.20)\ntrain_txt = train_x['cleaned_comment']\nval_txt = val_x['cleaned_comment']","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:08:06.019335Z","iopub.execute_input":"2021-12-07T18:08:06.019636Z","iopub.status.idle":"2021-12-07T18:08:07.401193Z","shell.execute_reply.started":"2021-12-07T18:08:06.019594Z","shell.execute_reply":"2021-12-07T18:08:07.400441Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=6000)\ntokenizer.fit_on_texts(train_txt)\ncnn_train = tokenizer.texts_to_sequences(train_txt)\ncnn_val = tokenizer.texts_to_sequences(val_txt)\nvocab_size = len(tokenizer.word_index) + 1  \nprint(f\"Vocab size:{vocab_size}\")","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:08:07.402344Z","iopub.execute_input":"2021-12-07T18:08:07.402619Z","iopub.status.idle":"2021-12-07T18:08:29.752001Z","shell.execute_reply.started":"2021-12-07T18:08:07.402583Z","shell.execute_reply":"2021-12-07T18:08:29.751224Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Pad sequences","metadata":{}},{"cell_type":"code","source":"maxlen = 100\nXcnn_train = pad_sequences(cnn_train, padding='post', maxlen=maxlen)\nXcnn_val = pad_sequences(cnn_val, padding='post', maxlen=maxlen)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:08:29.753234Z","iopub.execute_input":"2021-12-07T18:08:29.753697Z","iopub.status.idle":"2021-12-07T18:08:35.539992Z","shell.execute_reply.started":"2021-12-07T18:08:29.753653Z","shell.execute_reply":"2021-12-07T18:08:35.539253Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# build CNN model","metadata":{}},{"cell_type":"code","source":"embedding_dim = 200\ncnn_model = Sequential()\ncnn_model.add(Embedding(vocab_size, embedding_dim, input_length=maxlen))\ncnn_model.add(Conv1D(128, 5,activation = 'relu'))\ncnn_model.add(GlobalMaxPooling1D())\ncnn_model.add(Dense(16, activation='relu'))\ncnn_model.add(Dense(8, activation='relu'))\ncnn_model.add(Dense(1, activation='sigmoid'))\ncnn_model.compile(optimizer='adam',\n            loss='binary_crossentropy',\n            metrics=['accuracy'])\ncnn_model.summary() ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:08:35.542167Z","iopub.execute_input":"2021-12-07T18:08:35.542365Z","iopub.status.idle":"2021-12-07T18:08:35.963797Z","shell.execute_reply.started":"2021-12-07T18:08:35.542340Z","shell.execute_reply":"2021-12-07T18:08:35.963046Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"cnn_model.fit(Xcnn_train, train_y,\n                    epochs=3,\n                    verbose=True,\n                    validation_data=(Xcnn_val, val_y),\n                    batch_size=10)\nloss, accuracy = cnn_model.evaluate(Xcnn_train, train_y, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = cnn_model.evaluate(Xcnn_val, val_y, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy)) ","metadata":{"execution":{"iopub.status.busy":"2021-12-07T18:08:35.965158Z","iopub.execute_input":"2021-12-07T18:08:35.966734Z","iopub.status.idle":"2021-12-07T18:49:16.085570Z","shell.execute_reply.started":"2021-12-07T18:08:35.966687Z","shell.execute_reply":"2021-12-07T18:49:16.084822Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# LSTM","metadata":{}},{"cell_type":"code","source":"# stops_1 = {x for x in stops if len(x)<=3} - {'no','not','nor','off','own'}\nstops_1 = {}\ndef clean_text(comment):\n    text = str(comment)\n    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\), ]|(?:%[0-9a-fA-F][0-9a-fA-F]))+',' ',text)\n    text = re.sub(\"<.*?>\", \" \", text)\n    text = re.sub(r\"[0-9]+\",\" \",text)\n    text = re.sub(r\"@[A-Za-z0-9]+\",\" \",text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"can\\'t\", \"can not\", text)\n    text = re.sub(r\"n\\'t\", \" not\", text)\n    text = re.sub(r\"\\'re\", \" are\", text)\n    text = re.sub(r\"\\'s\", \" is\", text)\n    text = re.sub(r\"\\'d\", \" would\", text)\n    text = re.sub(r\"\\'ll\", \" will\", text)\n    text = re.sub(r\"\\'t\", \" not\", text)\n    text = re.sub(r\"\\'ve\", \" have\", text)\n    text = re.sub(r\"\\'m\", \" am\", text)\n    text = re.sub(r\"n\\'t\", ' not',text)\n    text = text.replace('\\\\r', ' ')\n    text = text.replace('\\\\\"', ' ')\n    text = text.replace('\\\\n', ' ')\n    text = re.sub('[^A-Za-z0-9]+',' ', text)\n    tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n    text = ' '.join(token for token in tokenizer.tokenize(text.lower()) if token not in stops_1)\n    text = text.lower().strip()\n    return text\n\ndf[\"cleaned_comment_1\"] = df.swifter.apply(lambda x: clean_text(x[\"comment\"]),axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T21:50:59.774963Z","iopub.execute_input":"2021-12-07T21:50:59.775237Z","iopub.status.idle":"2021-12-07T21:53:50.731175Z","shell.execute_reply.started":"2021-12-07T21:50:59.775206Z","shell.execute_reply":"2021-12-07T21:53:50.730257Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"corpus = [df['cleaned_comment_1'][i] for i in range( len(df))]\nvoc_size=5000\n\nonehot_=[one_hot(words,voc_size)for words in corpus] \n\nmax_sent_length = 80\n\nembedded_docs=pad_sequences(onehot_,padding='pre',maxlen=max_sent_length)\n    \nembedding_vector_features=80\n\nX_final=np.array(embedded_docs)\ny_final=np.array(df['label'])\n\nX_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_final, y_final, test_size=0.20, random_state=123)\n","metadata":{"execution":{"iopub.status.busy":"2021-12-07T21:53:50.733352Z","iopub.execute_input":"2021-12-07T21:53:50.733662Z","iopub.status.idle":"2021-12-07T21:54:18.863377Z","shell.execute_reply.started":"2021-12-07T21:53:50.733623Z","shell.execute_reply":"2021-12-07T21:54:18.862612Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"lstm_model=Sequential()\nlstm_model.add(Embedding(voc_size,embedding_vector_features,input_length=max_sent_length))\nlstm_model.add(Bidirectional(LSTM(128)))\nlstm_model.add(Dropout(0.3))\nlstm_model.add(Flatten())\nlstm_model.add(Dense(1,activation='sigmoid'))\nlstm_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nlstm_model.summary()","metadata":{"execution":{"iopub.status.busy":"2021-12-07T21:54:18.866287Z","iopub.execute_input":"2021-12-07T21:54:18.866577Z","iopub.status.idle":"2021-12-07T21:54:19.319147Z","shell.execute_reply.started":"2021-12-07T21:54:18.866522Z","shell.execute_reply":"2021-12-07T21:54:19.318258Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"lstm_model.fit(X_train_lstm,y_train_lstm,validation_data=(X_test_lstm,y_test_lstm),epochs=10,batch_size=10)","metadata":{"execution":{"iopub.status.busy":"2021-12-07T21:54:19.321309Z","iopub.execute_input":"2021-12-07T21:54:19.321826Z","iopub.status.idle":"2021-12-08T00:18:43.386146Z","shell.execute_reply.started":"2021-12-07T21:54:19.321782Z","shell.execute_reply":"2021-12-08T00:18:43.385448Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"loss, accuracy = lstm_model.evaluate(X_train_lstm, y_train_lstm, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = lstm_model.evaluate(X_test_lstm, y_test_lstm, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy)) ","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:26:35.764179Z","iopub.execute_input":"2021-12-08T00:26:35.764511Z","iopub.status.idle":"2021-12-08T00:28:45.157167Z","shell.execute_reply.started":"2021-12-08T00:26:35.764472Z","shell.execute_reply":"2021-12-08T00:28:45.156376Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}